{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb76e75d",
   "metadata": {},
   "source": [
    "# ChargeUp! Battery Geospatial Data Analysis and BSS Location Optimisation (Public) v1.0.0\n",
    "\n",
    "This notebook provides a framework for the geospatial analysis of e-motorcycle battery location data and the optimisation of battery swap station (BSS) locations, developed as part of the **ChargeUp!** project (2022-2023), funded by **P4G** (https://p4gpartnerships.org/chargeup). \n",
    "\n",
    "Author: Cameron Sheehan (Research Associate, Energy Futures Lab, Imperial College London)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9757f9d",
   "metadata": {},
   "source": [
    "## 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from keplergl import KeplerGl\n",
    "import random\n",
    "from h3 import h3\n",
    "import h3pandas\n",
    "from folium import Map, Marker, GeoJson\n",
    "from folium.plugins import MarkerCluster\n",
    "import branca.colormap as cm\n",
    "from branca.colormap import linear\n",
    "import folium\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import shapely\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "import pulp\n",
    "from pulp import LpMaximize, LpProblem, LpStatus, lpSum, LpVariable\n",
    "from descartes import PolygonPatch\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "import movingpandas as mpd\n",
    "import xarray as xr\n",
    "import hvplot.xarray  # noqa\n",
    "import hvplot.pandas \n",
    "from holoviews import opts\n",
    "import seaborn as sns\n",
    "from pytz import common_timezones, all_timezones\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd735d76",
   "metadata": {},
   "source": [
    "## 2. Choose to run notebook in either demo or production mode\n",
    "\n",
    "Uncomment the required mode. Demo mode demonstrates the analysis notebook by using a smaller subset of the total dataset so the analyses doesn't take too long to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_mode = 'demo'\n",
    "# notebook_mode = 'production'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef03f80",
   "metadata": {},
   "source": [
    "## 3. Import and pre-process input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd1d9e",
   "metadata": {},
   "source": [
    "### Open various data files as dataframes and process column data\n",
    "\n",
    "First, paste full pathname to the data files below, ensure pathname is inside inverted commas, i.e. '/XXX/XXX/XXX/battery-bms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data file pathnames\n",
    "pathname_battery_gps = ''\n",
    "pathname_battery_swaps = ''\n",
    "pathname_battery_bms = ''\n",
    "pathname_BSS_loc = ''\n",
    "pathname_population = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91debaf",
   "metadata": {},
   "source": [
    "Next, input the column headings for the required data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPS dataset\n",
    "gps_timestamp_col = ''\n",
    "batt_id_col = ''\n",
    "gps_lat_col = ''\n",
    "gps_lng_col = ''\n",
    "\n",
    "# For BSS locations dataset\n",
    "BSS_lat_col = ''\n",
    "BSS_lng_col = ''\n",
    "BSS_id_col = '';\n",
    "BSS_loc_name_col = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88b4d4",
   "metadata": {},
   "source": [
    "Read data from csv/parquet files and create pandas dataframes: <br>\n",
    "\n",
    "Note: Uncomment the correct option depending on file types csv/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba53a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv file and create a pandas dataframe\n",
    "df_raw = pd.read_csv(pathname_battery_gps, header=0) \n",
    "# df = pd.read_parquet(pathname_battery_gps) \n",
    "df = pd.DataFrame()\n",
    "\n",
    "df_existing_BSS_loc_raw = pd.read_csv(pathname_BSS_loc, header=0)\n",
    "# df_existing_BSS_loc = pd.read_parquet(pathname_BSS_loc)\n",
    "df_existing_BSS_loc = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d014676",
   "metadata": {},
   "source": [
    "If the Notebook is being run in 'demo' mode, create a smaller subset of the raw GPS data to be used in the analyis. Change the number to suit your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88909f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_mode == 'demo':\n",
    "    df_raw = df_raw.iloc[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cae54",
   "metadata": {},
   "source": [
    "Assign data columns to correct dataframe column headings for required data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85549f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['gps_timestamp','batt_id','gps_lat','gps_lng']] = df_raw[[gps_timestamp_col,batt_id_col,gps_lat_col,gps_lng_col]]\n",
    "\n",
    "df_existing_BSS_loc[['BSS_id','BSS_loc_name','BSS_lat','BSS_lng']] = df_existing_BSS_loc_raw[[BSS_id_col,BSS_loc_name_col,BSS_lat_col,BSS_lng_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e66f10",
   "metadata": {},
   "source": [
    "Process data types and required timezones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process battery_gps data\n",
    "df['batt_id'] = df['batt_id'].astype(\"string\")\n",
    "df['gps_time'] = pd.to_datetime(df['gps_timestamp'], unit='s', utc=True).dt.tz_convert('Africa/Nairobi')\n",
    "df = df.set_index('gps_time')\n",
    "\n",
    "# Create geodataframes from dataframes. Ensure co-ordinate reference system (crs) is correct. \n",
    "# Note EPSG 4326 is WGS 84 (https://epsg.io/4326)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.gps_lng, df.gps_lat, crs=\"EPSG:4326\"), crs=\"EPSG:4326\")\n",
    "\n",
    "# Process existing_BSS_loc data\n",
    "gdf_existing_BSS_loc = gpd.GeoDataFrame(\n",
    "    df_existing_BSS_loc, geometry=gpd.points_from_xy(df_existing_BSS_loc.BSS_lng, df_existing_BSS_loc.BSS_lat, crs=\"EPSG:4326\"), crs=\"EPSG:4326\")\n",
    "\n",
    "# Display first 5 lines of geodataframe to inspect data\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 lines of geodataframe to inspect data\n",
    "gdf_existing_BSS_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of each column. Ensure 'geometry' column is 'geometry' data type.\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35accab6",
   "metadata": {},
   "source": [
    "Determine the total number of unique batteries in the GPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batteries = len(gdf.batt_id.unique())\n",
    "print(\"There are a total of \", n_batteries, \" unique batteries, and \", len(gdf), \" data entries in this BMS dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274860a",
   "metadata": {},
   "source": [
    "#### Import Nairobi boundaries and other relevant map data from OpenStreetMaps using OSMNX package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Nairobi boundary and add a small buffer to allow trips that were just outside of the border to be included in the analysis\n",
    "NBO_dist_bndry = gpd.GeoDataFrame(geometry = ox.geocode_to_gdf('Nairobi').geometry.buffer(0.02));\n",
    "# Import Nairobi National Park boundary as an area to be removed from the anlaysis region\n",
    "NBO_nat_park_bndry = gpd.GeoDataFrame(geometry = ox.geocode_to_gdf('Nairobi national park').geometry)\n",
    "# Removed Nairobi National Park from the anlaysis region\n",
    "NBO_dist_bndry['geometry'] = NBO_dist_bndry.difference(NBO_nat_park_bndry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bcec9d",
   "metadata": {},
   "source": [
    "#### Display analysis boundary on Kepler gl map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bound = KeplerGl(height=800)\n",
    "# m.add_data(gdf.copy(), 'All GPS Points') # uncomment to add GPS points to map - then add new layer in Kepler gl\n",
    "m_bound.add_data(NBO_dist_bndry.copy(), 'Nairobi Boundary (excluding national parks)')\n",
    "%run 'Kepler configs/map_config_bound.py'\n",
    "m_bound.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925fef3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9e87a",
   "metadata": {},
   "source": [
    "Save map configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_bound.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_bound.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef90271",
   "metadata": {},
   "source": [
    "## 4. Create H3 hexagon dataframe for Nairobi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57509baa",
   "metadata": {},
   "source": [
    "#### Set H3 resolution and input edge length for analysis - details: https://h3geo.org/docs/core-library/restable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab219a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set H3 resolution\n",
    "res = 9\n",
    "# Find and input the hexagon edge length for the set H3 resolution from https://h3geo.org/docs/core-library/restable/#edge-lengths \n",
    "h3_hex_edge_length = 0.174375668 # km for res = 9\n",
    "\n",
    "# Calculate other hex dimensions based on edge length \n",
    "h3_hex_across_corners = 2*h3_hex_edge_length\n",
    "h3_hex_across_flats = math.sqrt(3)*h3_hex_edge_length\n",
    "# Set number (k) of hexagon rings for analysis (max of 5)\n",
    "k_rings = 5\n",
    "\n",
    "print(\"The following parameters have been set for the H3 hexagons: \\n H3 Resolution:\", res, \n",
    "      \"\\n H3 Hexagon distance across flats / between centres: \", np.round(h3_hex_across_flats*1000, decimals=1), \" m\",\n",
    "     \"\\n Number of hexagon 'rings' around each hexagon to use for analysis: \", k_rings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31baec",
   "metadata": {},
   "source": [
    "#### Fill Nairobi boundary with H3 hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77489f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBO_h3 = NBO_dist_bndry.h3.polyfill(res, explode=True).reset_index(drop=True).drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ba35d",
   "metadata": {},
   "source": [
    "#### Determine which H3 hexagons the existing BSSs are located in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_existing_BSS_h3 = gdf_existing_BSS_loc.h3.geo_to_h3(res)\n",
    "# Create a list of the unique H3 hex's where existing BSSs are located\n",
    "existing_BSS_h3_list = gdf_existing_BSS_h3.index.unique().to_list()\n",
    "# Create geometry of H3 hexagons that the existing BSSs are located in\n",
    "gdf_existing_BSS_h3_hex = gdf_existing_BSS_h3.h3.h3_to_geo_boundary().reset_index(drop=True)\n",
    "# Display the list of H3 indexes where BSS are located. \n",
    "# Note that the list may be shorter than the number of existing BSSs if some BSSs are located in the same H3 hex.\n",
    "existing_BSS_h3_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4f931",
   "metadata": {},
   "source": [
    "## 5. Analyse movemement/trip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d4af4",
   "metadata": {},
   "source": [
    "#### Create a movingpandas function to analyse gps movemement data:\n",
    "\n",
    "Function adapted from (Vallejo, B.R., 2021): https://towardsdatascience.com/stop-detection-in-blue-whales-gps-tracking-movingpandas-0-6-55a4b893a592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_point_traj_collection(moving_df, traject_id_column, minutes, searchradio):\n",
    "    ''' Parameters\n",
    "    - moving_df <geodataframe>: geodataframe of moving data formated with DateTime index\n",
    "    - traject_id_column <string>: column with tag id of individuals\n",
    "    - minutes <number>: minutes considered as minimum for the stop detection\n",
    "    - searchradio <number>: radio in meters considered for the stop detection\n",
    "    \n",
    "    Return\n",
    "    - <geodataframe>: stops as point with time duration and tag id of individuals\n",
    "    '''\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    all_stop_points = gpd.GeoDataFrame()\n",
    "    \n",
    "    # create a traj collection with movingpandas\n",
    "    traj_collection = mpd.TrajectoryCollection(moving_df, traject_id_column)\n",
    "    \n",
    "    for i in range(len(traj_collection)):\n",
    "\n",
    "        # create a stop detector\n",
    "        detector = mpd.TrajectoryStopDetector(traj_collection.trajectories[i])\n",
    "        \n",
    "        # stop points\n",
    "        stop_points = detector.get_stop_points(min_duration=timedelta(minutes=minutes), max_diameter=searchradio)\n",
    "        \n",
    "        # add duration to table\n",
    "        stop_points['duration_m'] = (stop_points['end_time']-stop_points['start_time']).dt.total_seconds()/60\n",
    "        \n",
    "        # add ID\n",
    "        stop_points['tag_id'] = [tag.split('_')[0] for tag in stop_points.index.to_list()]\n",
    "        \n",
    "        all_stop_points= all_stop_points.append(stop_points)\n",
    "        \n",
    "    return all_stop_points, traj_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f4a6e",
   "metadata": {},
   "source": [
    "### Determine all trajectories and stop points\n",
    "\n",
    "#### Set stop detection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stop search diameter (m) for detecting stops\n",
    "stop_diameter = 200;\n",
    "# Set minimum stop duration (min) for detecting stops\n",
    "stop_time_min = 5\n",
    "# Set minimum length of trajectories (m) to keep\n",
    "traj_length_min = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31318c0b",
   "metadata": {},
   "source": [
    "#### Apply function to data to determine all trajectories and stop points\n",
    "\n",
    "Note that this may take a few minutes to complete depending on the number of GPS points to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030d7c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "em_stops, em_traj = get_stop_point_traj_collection(gdf, 'batt_id', stop_time_min, stop_diameter) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407cd09",
   "metadata": {},
   "source": [
    "#### Split trajectories at stopping points\n",
    "\n",
    "Note that this may take a few minutes to complete depending on the number of trajectories to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Split trajectories based on stopping times, \n",
    "# Stops are detected where the GPS location stays within a set maximum diameter for a longer than a set minimum duration of time \n",
    "# Ignore trajectories with a length smaller than the set minimum length.\n",
    "split = mpd.StopSplitter(em_traj).split(max_diameter=stop_diameter, min_duration=timedelta(minutes=stop_time_min), min_length=traj_length_min)\n",
    "# Create geodataframe from of split trajectories\n",
    "gdf_traj = split.to_traj_gdf()\n",
    "# Convert trip length from meters to km\n",
    "gdf_traj['length_km']=gdf_traj['length']/1000\n",
    "\n",
    "# Display resulting geodataframe\n",
    "gdf_traj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b7c73",
   "metadata": {},
   "source": [
    "### Determine various trip statistics\n",
    "\n",
    "#### Trip distances and number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_traj['length_km'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine total trip length in km\n",
    "total_trip_distance = gdf_traj['length_km'].sum();\n",
    "total_no_of_trips = len(gdf_traj);\n",
    "print(\"The total number of trips and total distance of all trips detected in this dataset is: \\n \", total_no_of_trips,\" trips \\n\", np.round(total_trip_distance), \" km \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine average (mean) trip length in km\n",
    "mean_trip_distance = gdf_traj['length_km'].mean();\n",
    "print(\"The average trip distance of all trips detected in this dataset is: \\n \", np.round(mean_trip_distance, decimals=2), \" km \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47339bb9",
   "metadata": {},
   "source": [
    "#### Histogram of trip distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082b957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gdf_traj['length_km'].plot.hist(bins=np.arange(0,30,1), xlim=[-0.5,30], alpha=1, color=\"blue\", edgecolor = \"white\", figsize=(8,4));# ylim=[0,1000], \n",
    "# ax.set_title('Histogram of trip distances');\n",
    "ax.set_title('');\n",
    "ax.set_xlabel(\"Trip distance (km)\");\n",
    "ax.set_ylabel(\"Number of trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ec8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.figure.savefig(\"images/Histogram of trip distances.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05ff8",
   "metadata": {},
   "source": [
    "#### Determine trip average speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_traj['trip_duration'] = (gdf_traj['end_t'] - gdf_traj['start_t']).dt.total_seconds()/60\n",
    "gdf_traj['speed_avg'] = gdf_traj['length_km']/(gdf_traj['trip_duration']/60)\n",
    "gdf_traj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9b790",
   "metadata": {},
   "source": [
    "#### Histogram of trip average speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6f2be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gdf_traj['speed_avg'].plot.hist(bins=np.arange(0,75,5), alpha=1, color=\"blue\", edgecolor = \"white\", figsize=(8,4)); #bins=np.arange(0,30,1), xlim=[-0.5,30], ylim=[0,1000],\n",
    "# ax.set_title('Histogram of trip average speeds');\n",
    "ax.set_title('');\n",
    "ax.set_xlabel(\"Trip average speed (km/h)\");\n",
    "ax.set_ylabel(\"Number of trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.figure.savefig(\"images/Histogram of trip average speeds.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_avg = gdf_traj['speed_avg'].mean()\n",
    "speed_median = gdf_traj['speed_avg'].median()\n",
    "print(\"The average trip speed of all trips detected in this dataset is: \\n \", np.round(speed_avg, decimals=1), \" km/h   \\n\",\n",
    "     \"The median trip speed of all trips detected in this dataset is: \\n \", np.round(speed_median, decimals=1), \" km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbbe23",
   "metadata": {},
   "source": [
    "#### Investigate start times of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b17757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time increments for various analyses (note: this method ignores seconds when categorising time increments)\n",
    "\n",
    "gdf_traj['hr_inc'] = gdf_traj['start_t'].dt.hour + np.ceil(gdf_traj['start_t'].dt.minute/60)\n",
    "gdf_traj.loc[gdf_traj['hr_inc']==0, 'hr_inc'] = 24\n",
    "gdf_traj['hr_inc'] = gdf_traj['hr_inc'].astype('int')\n",
    "\n",
    "gdf_traj['30_mins_inc'] = gdf_traj['start_t'].dt.hour + np.ceil(gdf_traj['start_t'].dt.minute/30)*30/60\n",
    "gdf_traj.loc[gdf_traj['30_mins_inc']==0, '30_mins_inc'] = 24\n",
    "\n",
    "gdf_traj['15_mins_inc'] = gdf_traj['start_t'].dt.hour + np.ceil(gdf_traj['start_t'].dt.minute/15)*15/60\n",
    "gdf_traj.loc[gdf_traj['15_mins_inc']==0, '15_mins_inc'] = 24\n",
    "\n",
    "gdf_traj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c33ca",
   "metadata": {},
   "source": [
    "#### Histogram of trip start times (30 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1a28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_traj_plot = gdf_traj\n",
    "# Reduce the increment by 0.01 to ensure that the manually created increments are binned correctly \n",
    "# Histogram bins are exclsuive of the right side bin edge, so a value fo 1.5 would be incorrectly binned between 1.5-2.0, instead of 1.0-1.5\n",
    "gdf_traj_plot['30_mins_inc'] = gdf_traj_plot['30_mins_inc']-0.01 \n",
    "\n",
    "ax = gdf_traj_plot['30_mins_inc'].plot.hist(bins=np.arange(0,24.5,0.5),xticks=np.arange(0,25,1), alpha=1, color=\"blue\", edgecolor = \"white\", figsize=(8,3)); #bins=np.arange(0,30,1), xlim=[-0.5,30], ylim=[0,1000], [0,3,6,9,12,15,18,21,24]\n",
    "# ax.set_title('Histogram of trip start times');\n",
    "ax.set_title('');\n",
    "ax.set_xlabel(\"Trip start time period (hours)\");\n",
    "ax.set_ylabel(\"Number of trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84240695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.figure.savefig(\"images/Histogram of trip start times (30 mins).png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721a41e",
   "metadata": {},
   "source": [
    "#### Histogram of trip start times (hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a5826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce the increment by 0.01 to ensure that the manually created increments are binned correctly \n",
    "# Histogram bins are exclsuive of the right side bin edge, so a value fo 1.5 would be incorrectly binned between 1.5-2.0, instead of 1.0-1.5\n",
    "gdf_traj_plot['hr_inc'] = gdf_traj_plot['hr_inc']-0.01 \n",
    "\n",
    "ax = gdf_traj_plot['hr_inc'].plot.hist(bins=np.arange(0,25,1),xticks=np.arange(0,25,1), alpha=1, color=\"blue\", edgecolor = \"white\", figsize=(8,3)); #bins=np.arange(0,30,1), xlim=[-0.5,30], ylim=[0,1000], [0,3,6,9,12,15,18,21,24]\n",
    "# ax.set_title('Histogram of trip start times');\n",
    "ax.set_title('');\n",
    "ax.set_xlabel(\"Trip start time period (hours)\");\n",
    "ax.set_ylabel(\"Number of trips\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.figure.savefig(\"images/Histogram of trip start times (hours).png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b8ebd",
   "metadata": {},
   "source": [
    "## 6. Aggregate trip data using H3 hexagons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83586b86",
   "metadata": {},
   "source": [
    "#### Remove datetime format from gdf and stops indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c19431",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.reset_index(drop=True)\n",
    "em_stops= em_stops.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e89462",
   "metadata": {},
   "source": [
    "#### Add longitude and latitude columns for stop points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b674c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_stops['Lng'] = em_stops['geometry'].x\n",
    "em_stops['Lat'] = em_stops['geometry'].y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5202b74",
   "metadata": {},
   "source": [
    "#### Determine H3 hex indexes of all GPS data points and stopping locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d268f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the H3 hex indices\n",
    "hex_col = 'hex'+str(res)\n",
    "\n",
    "# find hexs containing the points\n",
    "gdf[hex_col] = gdf.apply(lambda x: h3.geo_to_h3(x.gps_lat,x.gps_lng,res),1)\n",
    "em_stops[hex_col] = em_stops.apply(lambda x: h3.geo_to_h3(x.Lat,x.Lng,res),1)\n",
    "\n",
    "# aggregate the points\n",
    "gdf_g = gdf.groupby(hex_col).size().to_frame('count_val').reset_index()\n",
    "gdf_g = gdf_g.set_index(hex_col).h3.h3_to_geo_boundary()\n",
    "em_stops_g = em_stops.groupby(hex_col).size().to_frame('count_val').reset_index()\n",
    "em_stops = em_stops.drop(columns=[hex_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94289b",
   "metadata": {},
   "source": [
    "#### Create hexagon geometries from H3 index values for analysis region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2028b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBO_h3 = NBO_h3.set_index('h3_polyfill').h3.h3_to_geo_boundary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4feb57f",
   "metadata": {},
   "source": [
    "#### Determine which H3 hex geometries are intersected with trips, then aggregate trip counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_intersects = gpd.sjoin(NBO_h3, gdf_traj, how=\"inner\", predicate='intersects')\n",
    "hex_intersects['const']=1\n",
    "\n",
    "# Group all H3 intersection hexes by their index values and count them\n",
    "hex_intersects_g = hex_intersects.groupby(['h3_polyfill']).size().to_frame('count').reset_index() \n",
    "\n",
    "# Display resulting dataframe with H3 indexes and trip counts\n",
    "hex_intersects_g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cda72",
   "metadata": {},
   "source": [
    "#### Merge the H3 hexes that have trip intersections with the rest of the Nairobi H3 hexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83112dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_intersects_merged = NBO_h3.merge(hex_intersects_g, how='left', on='h3_polyfill').fillna(0)\n",
    "\n",
    "# Display merged H3 hex dataframe with trip counts for all hexes\n",
    "hex_intersects_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6dc1ca",
   "metadata": {},
   "source": [
    "### Visualise H3 Hex trip heatmap with trip trajectories\n",
    "\n",
    "#### *Important Data Protection Note: This map may contain potentially sensitive location data of individual trips and stops. Depending on the contents of this map, it should not be published publicly without the explicit consent of all individuals whose battery GPS data was included in the analysed dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c84b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_heat = KeplerGl(height=800)\n",
    "hex_intersects_merged_m = hex_intersects_merged[['geometry','count']]\n",
    "m_heat.add_data(hex_intersects_merged_m.copy(), 'Merged Hex aggregated')\n",
    "# m_heat.add_data(gdf_traj[['traj_id','geometry','length_km']].copy(), 'GPS Trip Trajectories - split') # Uncomment to include individual trips - Add as new Kepler gl layer\n",
    "# m_heat.add_data(em_stops.copy(), 'Stop points') # Uncomment to include stop points  - Add as new Kepler gl layer\n",
    "m_heat.add_data(NBO_h3.copy(),'All H3 hexes')\n",
    "# load the config\n",
    "%run 'Kepler configs/map_config_heat.py'\n",
    "m_heat.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957049e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_heat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e2e7c",
   "metadata": {},
   "source": [
    "Save map configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdccda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_heat.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_heat.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76dac3c",
   "metadata": {},
   "source": [
    "### Create groups of \"neighbouring\" hexagons to be used in optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ecf91",
   "metadata": {},
   "source": [
    "#### Create lists of H3 indices for location hexagons and \"extra\" hex_ring hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d99996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of dataframe to the h3 index values created during the H3 polyfill process\n",
    "NBO_h3 = hex_intersects_merged.set_index('h3_polyfill')\n",
    "# Create a list of \"buffer\" hexagons to be used later \n",
    "NBO_h3_buff_list = NBO_h3\n",
    "# Use the H3 k_ring method to create a total of \"k\" rings\n",
    "NBO_h3_buff_list = NBO_h3_buff_list.h3.k_ring(k_rings, explode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where the h3_k_ring index values are repeated\n",
    "NBO_h3_buff_list = NBO_h3_buff_list.drop_duplicates(subset=['h3_k_ring'], keep='first')\n",
    "# Create list from the h3_k_ring index values\n",
    "NBO_h3_buff_list = NBO_h3_buff_list.h3_k_ring.to_list()\n",
    "# Create list of h3 index values that are in the analysis region\n",
    "NBO_h3_analysis_list = NBO_h3.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df076746",
   "metadata": {},
   "source": [
    "#### Remove analysis region hex indices from \"buffer\" hex ring list, i.e. leave ONLY extra neighbor hexagons that are not part of the analysis set of hexagons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in NBO_h3_buff_list[:]:\n",
    "        if i in NBO_h3_analysis_list:\n",
    "            NBO_h3_buff_list.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f692e7",
   "metadata": {},
   "source": [
    "#### Add H3 hex_ring column to Nairobi locations H3 dataframe, to indicate hex neighbours for each index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range (1, k_rings+1):\n",
    "    NBO_h3 = NBO_h3.h3.hex_ring(k)\n",
    "    NBO_h3.rename(columns={'h3_hex_ring':'h3_hex_ring_'+str(k)}, inplace = True)\n",
    "    \n",
    "NBO_h3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe09a1",
   "metadata": {},
   "source": [
    "#### Remove neighbour indices from hex_ring column that are outdside of the analysis boundary set (i.e. remove \"extra\" hexagons)\n",
    "\n",
    "Note: This step may take a few mins to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_indices = NBO_h3_buff_list\n",
    "for k in range (1, k_rings+1):\n",
    "    for i in range(len(NBO_h3['h3_hex_ring_'+str(k)])):\n",
    "        for r in remove_indices:\n",
    "            if r in NBO_h3['h3_hex_ring_'+str(k)][i]:\n",
    "                NBO_h3['h3_hex_ring_'+str(k)][i].remove(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87340a5",
   "metadata": {},
   "source": [
    "### Add population data from GeoTiff to H3 hexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBO_h3['pop'] = NBO_h3['geometry'].apply(lambda x: zonal_stats(x, pathname_population, stats=['sum'])[0]['sum']).fillna(0)\n",
    "NBO_h3 = NBO_h3.h3.cell_area()\n",
    "NBO_h3['pop_dens'] = NBO_h3['pop']/NBO_h3['h3_cell_area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92724480",
   "metadata": {},
   "source": [
    "## 7. Process the data for optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all counts below the set minimum count limit to zero\n",
    "min_trip_count = 10\n",
    "NBO_h3.loc[NBO_h3['count'] <= min_trip_count, 'count'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to values between zero and one [0,1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "NBO_h3['pop_scaled'] = min_max_scaler.fit_transform(NBO_h3['pop'].values.reshape(-1, 1)) #returns a numpy array\n",
    "NBO_h3['trips_scaled'] = min_max_scaler.fit_transform(NBO_h3['count'].values.reshape(-1, 1)) #returns a numpy array\n",
    "\n",
    "# Display dataframe\n",
    "NBO_h3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7463889",
   "metadata": {},
   "source": [
    "#### Set score weighting values (all weights must add up to 1.0)\n",
    "\n",
    "pop_w = Population score weighting <br>\n",
    "trips_w = Trip volume score weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_w = 1.0 # Set weight value of trips between 0 and 1 (0 - 100%)\n",
    "\n",
    "pop_w = 1.0 - trips_w # Weight value of population data calculated as remaining weight percentage\n",
    "score_w = {'pop_w':pop_w, 'trips_w':trips_w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine combined score based on weightings\n",
    "NBO_h3['score_comb'] = NBO_h3['pop_scaled']*score_w['pop_w'] + NBO_h3['trips_scaled']*score_w['trips_w']\n",
    "NBO_h3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92bb22",
   "metadata": {},
   "source": [
    "### Visualise H3 trip counts and population scores using Kepler.gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f79a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_trips_pop = KeplerGl(height=800)\n",
    "m_trips_pop.add_data(NBO_h3.copy(), 'Trip + Pop Data')\n",
    "# load the config\n",
    "%run 'Kepler configs/map_config_trips_pop.py'\n",
    "m_trips_pop.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f2e73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_trips_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8f889",
   "metadata": {},
   "source": [
    "Save map config to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662602b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_trips_pop.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_trips_pop.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3c0e1",
   "metadata": {},
   "source": [
    "## 8. Setup Mixed-Integer Linear Programming (MILP) Optimisation using PuLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdee6d0",
   "metadata": {},
   "source": [
    "### Set optimisation inputs for each stage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f701d3f",
   "metadata": {},
   "source": [
    "#### Stage 1 Optimisation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b57d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_01 = 10 # Number of BSSs to be built\n",
    "R_ex_01 = len(existing_BSS_h3_list) # Number of existing BSS sites\n",
    "w_01 = [1.0,0.5,0.3,0.2,0.1,0.05] # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M_01 = 1.0\n",
    "df_01 = NBO_h3\n",
    "df_01['coverage'] = 0\n",
    "n_rows_01 = len(df_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52136b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ex_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d80973",
   "metadata": {},
   "source": [
    "#### Stage 2 Optimisation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_02 = 10 # Number of BSSs to be built\n",
    "R_ex_02 = R_01 + R_ex_01 # Number of existing BSS sites\n",
    "w_02 = [1.0,0.5,0.3,0.2,0,0] # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M_02 = 1.0\n",
    "df_02 = NBO_h3\n",
    "df_02['coverage'] = 0\n",
    "n_rows_02 = len(df_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ex_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecaaa9",
   "metadata": {},
   "source": [
    "#### Stage 3 Optimisation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0266920",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_03 = 10 # Number of BSSs to be built\n",
    "R_ex_03 = R_02 + R_ex_02 # Number of existing BSS sites\n",
    "w_03 = [1.0,0.5,0,0,0,0] # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M_03 = 1.0\n",
    "df_03 = NBO_h3\n",
    "df_03['coverage'] = 0\n",
    "n_rows_03 = len(df_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530dece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ex_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e7a5d",
   "metadata": {},
   "source": [
    "### Stage 1 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R_01 # Number of BSSs to be built\n",
    "R_ex = R_ex_01  # Number of existing BSS sites\n",
    "existing_BSS_h3_list_updated = existing_BSS_h3_list # Existing BSS H3 hex list\n",
    "w = w_01  # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M = M_01\n",
    "df = df_01\n",
    "n_rows = n_rows_01 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49603d",
   "metadata": {},
   "source": [
    "#### Declare problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = LpProblem(\"max_bss_coverage\", LpMaximize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a923b",
   "metadata": {},
   "source": [
    "#### Define the decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decision'] = [LpVariable(name=\"y\"+str(i), cat='Binary') for i in range(n_rows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca67e2",
   "metadata": {},
   "source": [
    "#### Add constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['decision']]) <= R + R_ex   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56005e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(R_ex):\n",
    "    prob += df.loc[existing_BSS_h3_list_updated[j]].decision == 1\n",
    "    \n",
    "    # Note: If there is an error in this step, it may be because the existing station does not \n",
    "    # lie within the boundary selected for this analysis. Either increase the size of the boundary \n",
    "    # or remove the station causing the issue from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_rows):\n",
    "    prob +=  w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))  <= M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f108f0",
   "metadata": {},
   "source": [
    "#### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['score_comb'][i]*(w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))) for i in range(n_rows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477c189",
   "metadata": {},
   "source": [
    "#### Solve problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf568314",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.solve();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulp.LpStatus[prob.status]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78733e7",
   "metadata": {},
   "source": [
    "#### Create dataframe of Stage 1 Optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb531f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opt_site'] = [df['decision'][i].varValue for i in range(n_rows)]  \n",
    "df['coverage'] = [w[0]*df['opt_site'][i] + sum(sum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].opt_site) for k in range(1,k_rings+1)) for i in range(n_rows)]\n",
    "df['coverage_score'] = df['score_comb']*df['coverage']\n",
    "\n",
    "df_results_01 = df[['opt_site','coverage_score','coverage', 'count', 'pop', 'score_comb', 'geometry']]\n",
    "df_opt_sites_01 = df_results_01[df_results_01['opt_site']==1]\n",
    "gdf_opt_sites_01 = df_opt_sites_01.h3.h3_to_geo_boundary().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ff25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_opt_sites_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f70d3f",
   "metadata": {},
   "source": [
    "### Visualise optimal site results and existing BSS sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a3f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_stage1 = KeplerGl(height=800)\n",
    "m_stage1.add_data(df_results_01.copy(), 'Results')\n",
    "m_stage1.add_data(gdf_opt_sites_01.copy(), 'Optimal BSS sites')\n",
    "m_stage1.add_data(gdf_existing_BSS_loc.copy(), 'Existing BSS sites')\n",
    "m_stage1.add_data(gdf_existing_BSS_h3_hex.copy(), 'Existing BSS site H3 hexes')\n",
    "%run 'Kepler configs/map_config_stage1.py'\n",
    "m_stage1.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48898772",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_stage1.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_stage1.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a7307",
   "metadata": {},
   "source": [
    "### Stage 2 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R_02 # Number of BSSs to be built\n",
    "R_ex = R_ex_02  # Number of existing BSS sites\n",
    "w = w_02  # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M = M_02\n",
    "df = df_02\n",
    "n_rows = n_rows_02 \n",
    "existing_BSS_h3_list_updated = df_opt_sites_01.index.unique().to_list() # Existing BSS H3 hex list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fd19f",
   "metadata": {},
   "source": [
    "#### Declare problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = LpProblem(\"max_bss_coverage\", LpMaximize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e686089",
   "metadata": {},
   "source": [
    "#### Define the decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a364c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decision'] = [LpVariable(name=\"y\"+str(i), cat='Binary') for i in range(n_rows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b3934",
   "metadata": {},
   "source": [
    "#### Add constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2685f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['decision']]) <= R + R_ex   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6623ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(R_ex):\n",
    "    prob += df.loc[existing_BSS_h3_list_updated[j]].decision == 1\n",
    "    \n",
    "    # Note: If there is an error in this step, it may be because the existing station does not \n",
    "    # lie within the boundary selected for this analysis. Either increase the size of the boundary \n",
    "    # or remove the station causing the issue from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39344dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_rows):\n",
    "    prob +=  w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))  <= M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3021ab",
   "metadata": {},
   "source": [
    "#### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52771d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['score_comb'][i]*(w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))) for i in range(n_rows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01fc9c",
   "metadata": {},
   "source": [
    "#### Solve problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.solve();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulp.LpStatus[prob.status]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773aed0",
   "metadata": {},
   "source": [
    "#### Create dataframe of Stage 2 Optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da102ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opt_site'] = [df['decision'][i].varValue for i in range(n_rows)]  \n",
    "df['coverage'] = [w[0]*df['opt_site'][i] + sum(sum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].opt_site) for k in range(1,k_rings+1)) for i in range(n_rows)]\n",
    "df['coverage_score'] = df['score_comb']*df['coverage']\n",
    "\n",
    "df_results_02 = df[['opt_site','coverage_score','coverage', 'count', 'pop', 'score_comb', 'geometry']]\n",
    "df_opt_sites_02 = df_results_02[df_results_02['opt_site']==1]\n",
    "gdf_opt_sites_02 = df_opt_sites_02.h3.h3_to_geo_boundary().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fa35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_opt_sites_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a8888",
   "metadata": {},
   "source": [
    "### Visualise optimal site results and existing BSS sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf595d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_stage2 = KeplerGl(height=800)\n",
    "m_stage2.add_data(df_results_02.copy(), 'Results')\n",
    "m_stage2.add_data(gdf_opt_sites_02.copy(), 'Stage 2 Optimal BSS sites')\n",
    "m_stage2.add_data(gdf_opt_sites_01.copy(), 'Stage 1 Optimal BSS sites')\n",
    "m_stage2.add_data(gdf_existing_BSS_loc.copy(), 'Existing BSS sites')\n",
    "m_stage2.add_data(gdf_existing_BSS_h3_hex.copy(), 'Existing BSS site H3 hexes')\n",
    "%run 'Kepler configs/map_config_stage2.py'\n",
    "m_stage2.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d65970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_stage2.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_stage2.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962978f",
   "metadata": {},
   "source": [
    "### Stage 3 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d88a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R_03 # Number of BSSs to be built\n",
    "R_ex = R_ex_03  # Number of existing BSS sites\n",
    "w = w_03  # Coverage weightings for each ring of hexagons, first value is the centre hex\n",
    "M = M_03\n",
    "df = df_03\n",
    "n_rows = n_rows_03 \n",
    "existing_BSS_h3_list_updated = df_opt_sites_02.index.unique().to_list() # Existing BSS H3 hex list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af13f07",
   "metadata": {},
   "source": [
    "#### Declare problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4062811",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = LpProblem(\"max_bss_coverage\", LpMaximize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e5c36",
   "metadata": {},
   "source": [
    "#### Define the decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decision'] = [LpVariable(name=\"y\"+str(i), cat='Binary') for i in range(n_rows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c213e",
   "metadata": {},
   "source": [
    "#### Add constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['decision']]) <= R + R_ex   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5382ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(R_ex):\n",
    "    prob += df.loc[existing_BSS_h3_list_updated[j]].decision == 1\n",
    "    \n",
    "    # Note: If there is an error in this step, it may be because the existing station does not \n",
    "    # lie within the boundary selected for this analysis. Either increase the size of the boundary \n",
    "    # or remove the station causing the issue from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8922d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_rows):\n",
    "    prob +=  w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))  <= M                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90759935",
   "metadata": {},
   "source": [
    "#### Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c058da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob += lpSum([df['score_comb'][i]*(w[0]*df['decision'][i] + lpSum(lpSum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].decision) for k in range(1,k_rings+1))) for i in range(n_rows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee122017",
   "metadata": {},
   "source": [
    "#### Solve problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.solve();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulp.LpStatus[prob.status]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833d157",
   "metadata": {},
   "source": [
    "#### Create dataframe of Stage 3 Optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opt_site'] = [df['decision'][i].varValue for i in range(n_rows)]  \n",
    "df['coverage'] = [w[0]*df['opt_site'][i] + sum(sum(w[k]*df.loc[df['h3_hex_ring_'+str(k)][i]].opt_site) for k in range(1,k_rings+1)) for i in range(n_rows)]\n",
    "df['coverage_score'] = df['score_comb']*df['coverage']\n",
    "\n",
    "df_results_03 = df[['opt_site','coverage_score','coverage', 'count', 'pop', 'score_comb', 'geometry']]\n",
    "df_opt_sites_03 = df_results_03[df_results_03['opt_site']==1]\n",
    "gdf_opt_sites_03 = df_opt_sites_03.h3.h3_to_geo_boundary().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c714beb",
   "metadata": {},
   "source": [
    "### Visualise optimal site results and existing BSS sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad1702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_stage3 = KeplerGl(height=800)\n",
    "m_stage3.add_data(df_results_03.copy(), 'Results')\n",
    "m_stage3.add_data(gdf_opt_sites_01.copy(), 'Stage 1 Optimal BSS sites')\n",
    "m_stage3.add_data(gdf_opt_sites_02.copy(), 'Stage 2 Optimal BSS sites')\n",
    "m_stage3.add_data(gdf_opt_sites_03.copy(), 'Stage 3 Optimal BSS sites')\n",
    "m_stage3.add_data(gdf_existing_BSS_loc.copy(), 'Existing BSS sites')\n",
    "m_stage3.add_data(gdf_existing_BSS_h3_hex.copy(), 'Existing BSS site H3 hexes')\n",
    "m_stage3.add_data(NBO_h3['geometry'].reset_index().copy(),'All H3 hexes')\n",
    "%run 'Kepler configs/map_config_stage3.py'\n",
    "m_stage3.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb189af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Kepler configs/map_config_stage3.py', 'w') as f:\n",
    "#    f.write('config = {}'.format(m_stage3.config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
